{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l5xxcxd0vpW"
      },
      "source": [
        "# Using pre-trained CNN\n",
        "\n",
        "In this lab, we will see:\n",
        "\n",
        "- Zero-shot performance of pre-trained backbone\n",
        "- Use pre-trained CNN as backbone\n",
        "- Fine-tuning the pre-trained CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tkm5SfEo1c1r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6Oh2yVOi3Lie"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 0.01\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\") # to use the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XWJUU9yp0vHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3fa29f-b05d-415e-a558-6753b29d727b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "# create a split for train/validation. We can use early stop\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a pre-defined network with pretrained weights\n",
        "\n"
      ],
      "metadata": {
        "id": "mkiBrlvS1utb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uav1QxKwUXE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad662c4c-377b-4b4a-a58a-841dbcaf378e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 89.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "# override the fc layer of the network since it is of 1000 classes by default (ImageNet)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aWSNZ5Hf18Kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69b4fb4-9aea-44f0-e3a5-a295dd15274b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11181642"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# count the trainable parameters of the model\n",
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count_trainable_parameters(net)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen all the weights of the network, except for fc ones\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "net.fc.weight.requires_grad = True\n",
        "net.fc.bias.requires_grad = True\n",
        "count_trainable_parameters(net)"
      ],
      "metadata": {
        "id": "3uNOIDDs1_QD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be79ae20-9469-4864-96fd-425f3416dfed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5130"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UH7STI6m0dg9"
      },
      "outputs": [],
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-04)"
      ],
      "metadata": {
        "id": "b3SqSMwo4qp1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GqA_UXQZ2zc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc988149-211c-43af-c5e1-b05097f689bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.827748\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 2.448497\n",
            "\\Val set: Average loss: 331.4472, Accuracy: 3523/10000 (35%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 2.100048\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.932522\n",
            "\\Val set: Average loss: 333.1411, Accuracy: 3540/10000 (35%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 2.232368\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.597827\n",
            "\\Val set: Average loss: 338.2413, Accuracy: 3388/10000 (34%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 2.136764\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 2.050173\n",
            "\\Val set: Average loss: 309.4349, Accuracy: 3679/10000 (37%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.989875\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 2.635054\n",
            "\\Val set: Average loss: 346.1276, Accuracy: 3300/10000 (33%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 2.366601\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 2.508025\n",
            "\\Val set: Average loss: 324.6189, Accuracy: 3504/10000 (35%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 2.172338\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.742405\n",
            "\\Val set: Average loss: 327.5846, Accuracy: 3550/10000 (36%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.795587\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 1.761458\n",
            "\\Val set: Average loss: 315.0072, Accuracy: 3676/10000 (37%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 2.310194\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 2.003468\n",
            "\\Val set: Average loss: 337.2147, Accuracy: 3438/10000 (34%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 2.125901\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 1.664424\n",
            "\\Val set: Average loss: 321.2299, Accuracy: 3531/10000 (35%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# the main loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader, val=True)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4iwFGBFz6FUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc4813e-268b-4082-8104-9fe220144c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Test set: Average loss: 343.2623, Accuracy: 3477/10000 (35%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = test(net, device, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add additional layer to the pre-trained model\n"
      ],
      "metadata": {
        "id": "oo5XBPHC5ePQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fc1 = nn.Linear(512, 128)\n",
        "\n",
        "# Modify the existing fully connected layer (fc)\n",
        "net.fc = nn.Linear(128, 10)\n",
        "\n",
        "# Replace the model's classifier with a new sequential layer\n",
        "# that includes the new fc1 and the modified fc\n",
        "net.fc = nn.Sequential(\n",
        "    fc1,\n",
        "    nn.ReLU(),   # Optional: Add an activation function like ReLU\n",
        "    net.fc\n",
        ")\n",
        "net.to(device)"
      ],
      "metadata": {
        "id": "Z1UXzoW65drQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d939b0b0-bb1f-409e-e1db-e849624c8497"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning some part of the CNN (not only the classifier)"
      ],
      "metadata": {
        "id": "otJejt0A1f9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d4nO6OAdXsXW"
      },
      "outputs": [],
      "source": [
        "# Unfreeze layer4 parameters\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "# Setting different learning rates\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n",
        "\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer4_params, fc_params], momentum=0.9, weight_decay=1e-04)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(net, trainloader, optimizer, valloader, testloader, device):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    model_state_dict = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss, val_acc = test(net, device, valloader, val=True)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "    test_loss, test_acc = test(net, device, testloader)\n",
        "    return train_losses, val_losses, val_accuracies, test_loss, test_acc"
      ],
      "metadata": {
        "id": "3qrgmltML-yd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "502_-PVVMvqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38b5531-d016-4037-c2f9-1910118d9c88"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.315609\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 2.066176\n",
            "\\Val set: Average loss: 286.7457, Accuracy: 3449/10000 (34%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.550293\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.689312\n",
            "\\Val set: Average loss: 244.8259, Accuracy: 4245/10000 (42%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.908844\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.633983\n",
            "\\Val set: Average loss: 218.5828, Accuracy: 5079/10000 (51%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.688830\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.447590\n",
            "\\Val set: Average loss: 202.0690, Accuracy: 5539/10000 (55%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.258109\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.335482\n",
            "\\Val set: Average loss: 192.5636, Accuracy: 5813/10000 (58%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.258438\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.341215\n",
            "\\Val set: Average loss: 191.3492, Accuracy: 5800/10000 (58%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 1.481382\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.231183\n",
            "\\Val set: Average loss: 183.2410, Accuracy: 6012/10000 (60%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.097681\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 1.313750\n",
            "\\Val set: Average loss: 179.0106, Accuracy: 6155/10000 (62%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 1.175428\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.232515\n",
            "\\Val set: Average loss: 174.7068, Accuracy: 6202/10000 (62%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 1.106809\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 1.056388\n",
            "\\Val set: Average loss: 168.6194, Accuracy: 6304/10000 (63%)\n",
            "\n",
            "\\Test set: Average loss: 169.8140, Accuracy: 6293/10000 (63%)\n",
            "\n",
            "Test accuracy: 0.629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "How many layers it is better to fine-tune?\n",
        "\n",
        "It is better to update all the weights of the model?"
      ],
      "metadata": {
        "id": "kyOs4mx92koz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to add more and more layers to finetuning and check\n",
        "net = models.resnet18(pretrained=True)\n",
        "fc1 = nn.Linear(512, 128)\n",
        "\n",
        "# Modify the existing fully connected layer (fc)\n",
        "net.fc = nn.Linear(128, 10)\n",
        "\n",
        "# Replace the model's classifier with a new sequential layer\n",
        "# that includes the new fc1 and the modified fc\n",
        "net.fc = nn.Sequential(\n",
        "    fc1,\n",
        "    nn.ReLU(),   # Optional: Add an activation function like ReLU\n",
        "    net.fc\n",
        ")\n",
        "net.to(device)\n",
        "\n",
        "# Unfreeze layer4 parameters\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze layer3 parameters\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "# Setting different learning rates\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n",
        "layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001}\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n",
        "\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer4_params, layer3_params, fc_params], momentum=0.9, weight_decay=1e-04)"
      ],
      "metadata": {
        "id": "muMJ79zgDMQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293908ce-bb6a-4bb6-f69a-3589245e6a89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja0yZUD6DLyo",
        "outputId": "93aa6cad-eefc-4e3c-fbd9-d83f24b65183"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.354466\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.578512\n",
            "\\Val set: Average loss: 222.1107, Accuracy: 5037/10000 (50%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.642929\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.467282\n",
            "\\Val set: Average loss: 174.3838, Accuracy: 6211/10000 (62%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.282034\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.996403\n",
            "\\Val set: Average loss: 159.0620, Accuracy: 6583/10000 (66%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.751311\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.987950\n",
            "\\Val set: Average loss: 146.5400, Accuracy: 6771/10000 (68%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.976102\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.012379\n",
            "\\Val set: Average loss: 137.4820, Accuracy: 7009/10000 (70%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.895779\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.824644\n",
            "\\Val set: Average loss: 136.9796, Accuracy: 7037/10000 (70%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.836829\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.991552\n",
            "\\Val set: Average loss: 129.9981, Accuracy: 7133/10000 (71%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.877216\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.739469\n",
            "\\Val set: Average loss: 126.1389, Accuracy: 7218/10000 (72%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.850266\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.658610\n",
            "\\Val set: Average loss: 123.7831, Accuracy: 7278/10000 (73%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.923842\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.764904\n",
            "\\Val set: Average loss: 121.9613, Accuracy: 7380/10000 (74%)\n",
            "\n",
            "\\Test set: Average loss: 117.7501, Accuracy: 7503/10000 (75%)\n",
            "\n",
            "Test accuracy: 0.750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to add more and more layers to finetuning and check\n",
        "net = models.resnet18(pretrained=True)\n",
        "fc1 = nn.Linear(512, 128)\n",
        "\n",
        "# Modify the existing fully connected layer (fc)\n",
        "net.fc = nn.Linear(128, 10)\n",
        "\n",
        "# Replace the model's classifier with a new sequential layer\n",
        "# that includes the new fc1 and the modified fc\n",
        "net.fc = nn.Sequential(\n",
        "    fc1,\n",
        "    nn.ReLU(),   # Optional: Add an activation function like ReLU\n",
        "    net.fc\n",
        ")\n",
        "net.to(device)\n",
        "\n",
        "# Unfreeze layer4 parameters\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze layer3 parameters\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze layer2 parameters\n",
        "for param in net.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "# Setting different learning rates\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n",
        "layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001}\n",
        "layer2_params = {'params': net.layer2.parameters(), 'lr': 0.001}\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n",
        "\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer4_params, layer3_params, layer2_params, fc_params], momentum=0.9, weight_decay=1e-04)"
      ],
      "metadata": {
        "id": "qnMwvaHzDig_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_MXYw0sD9G_",
        "outputId": "e8889f98-0674-4957-88d8-dd0fb732d147"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.268929\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.617086\n",
            "\\Val set: Average loss: 200.9513, Accuracy: 5459/10000 (55%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.417658\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.153781\n",
            "\\Val set: Average loss: 147.7013, Accuracy: 6760/10000 (68%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.928498\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.034508\n",
            "\\Val set: Average loss: 131.3643, Accuracy: 7198/10000 (72%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.879185\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.894237\n",
            "\\Val set: Average loss: 120.4690, Accuracy: 7429/10000 (74%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.917641\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.782772\n",
            "\\Val set: Average loss: 117.3681, Accuracy: 7493/10000 (75%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.860103\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.654987\n",
            "\\Val set: Average loss: 112.1097, Accuracy: 7584/10000 (76%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.927552\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.519626\n",
            "\\Val set: Average loss: 103.0345, Accuracy: 7780/10000 (78%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.802130\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.566888\n",
            "\\Val set: Average loss: 103.8277, Accuracy: 7794/10000 (78%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.521907\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.543255\n",
            "\\Val set: Average loss: 101.6844, Accuracy: 7835/10000 (78%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.511361\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.470869\n",
            "\\Val set: Average loss: 101.3696, Accuracy: 7816/10000 (78%)\n",
            "\n",
            "\\Test set: Average loss: 96.6028, Accuracy: 7977/10000 (80%)\n",
            "\n",
            "Test accuracy: 0.798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to add more and more layers to finetuning and check\n",
        "net = models.resnet18(pretrained=True)\n",
        "fc1 = nn.Linear(512, 128)\n",
        "\n",
        "# Modify the existing fully connected layer (fc)\n",
        "net.fc = nn.Linear(128, 10)\n",
        "\n",
        "# Replace the model's classifier with a new sequential layer\n",
        "# that includes the new fc1 and the modified fc\n",
        "net.fc = nn.Sequential(\n",
        "    fc1,\n",
        "    nn.ReLU(),   # Optional: Add an activation function like ReLU\n",
        "    net.fc\n",
        ")\n",
        "net.to(device)\n",
        "\n",
        "# Unfreeze layer4 parameters\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze layer3 parameters\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze layer2 parameters\n",
        "for param in net.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze layer1 parameters\n",
        "for param in net.layer1.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "# Setting different learning rates\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n",
        "layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001}\n",
        "layer2_params = {'params': net.layer2.parameters(), 'lr': 0.001}\n",
        "layer1_params = {'params': net.layer1.parameters(), 'lr': 0.001}\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n",
        "\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer4_params, layer3_params, layer2_params, layer1_params, fc_params], momentum=0.9, weight_decay=1e-04)"
      ],
      "metadata": {
        "id": "eGboY-oFD_YP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnPgSNTYEAOR",
        "outputId": "e4ea37ce-4c82-488d-bf33-34d64be88199"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.328807\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.204330\n",
            "\\Val set: Average loss: 219.0783, Accuracy: 4824/10000 (48%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.674671\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.094626\n",
            "\\Val set: Average loss: 149.3416, Accuracy: 6770/10000 (68%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.647139\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.833735\n",
            "\\Val set: Average loss: 133.9396, Accuracy: 7075/10000 (71%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.796413\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.979233\n",
            "\\Val set: Average loss: 116.9873, Accuracy: 7509/10000 (75%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.763214\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.819676\n",
            "\\Val set: Average loss: 120.6367, Accuracy: 7470/10000 (75%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.052742\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.594537\n",
            "\\Val set: Average loss: 105.7238, Accuracy: 7742/10000 (77%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.585527\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.731748\n",
            "\\Val set: Average loss: 101.5547, Accuracy: 7884/10000 (79%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.518415\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.484246\n",
            "\\Val set: Average loss: 100.6822, Accuracy: 7869/10000 (79%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.540674\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.498355\n",
            "\\Val set: Average loss: 99.8254, Accuracy: 7877/10000 (79%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.557110\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.424318\n",
            "\\Val set: Average loss: 94.1711, Accuracy: 8004/10000 (80%)\n",
            "\n",
            "\\Test set: Average loss: 90.8844, Accuracy: 8101/10000 (81%)\n",
            "\n",
            "Test accuracy: 0.810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Try to change the hyper-parameters of the fine-tuning (e.g. lr of CNN layers and lr of the fc layers) and/or network architecture"
      ],
      "metadata": {
        "id": "cUunhER-2y9v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GdMVMg_DL4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Try to implement the model selection strategy (also known as early stopping) based on the validation accuracy on cifar10.\n",
        "\n",
        "Consider using the two following command to respectively save and load the state of all the parameters of the model in a moment."
      ],
      "metadata": {
        "id": "GuygmHB43UHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save all the parameters of the model\n",
        "model_state_dict = net.state_dict()\n",
        "\n",
        "# load saved weights on the model\n",
        "net.load_state_dict(model_state_dict)"
      ],
      "metadata": {
        "id": "uOhllTzr32Ld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95485633-cd28-49c5-e7aa-6b51d74eb818"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(model, device, train_loader, valloader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_corr = 0\n",
        "        for batch_idx, (data, target) in enumerate(valloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss_val = criterion(output, target)\n",
        "            val_losses.append(loss_val.item())\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            val_corr += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_accuracy = val_corr / 10000 # Validation set has 10000 elements\n",
        "    return np.mean(losses), np.mean(val_losses), val_accuracy"
      ],
      "metadata": {
        "id": "iRk9OPHzF2qk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18()\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-04)\n",
        "best_accuracy = 0\n",
        "model_state_dict = None\n",
        "for epoch in range(100):\n",
        "    train_loss, val_loss, val_acc = train_and_eval(net, device, trainloader, valloader, optimizer, epoch)\n",
        "    print(f\"Epoch {epoch + 1} mean train loss: {train_loss:.3f}\")\n",
        "    if val_acc > best_accuracy:\n",
        "        best_accuracy = val_acc\n",
        "        model_state_dict = net.state_dict()\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "net.load_state_dict(model_state_dict)\n",
        "test_loss, test_acc = test(net, device, testloader)\n",
        "print(f\"Test accuracy with early stopping: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "ScwmTTmzHSIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6db985-743b-47cf-c353-35b5a4b2e718"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/40000 (0%)]\tLoss: 2.592381\n",
            "Train Epoch: 0 [32000/40000 (80%)]\tLoss: 1.479216\n",
            "Epoch 1 mean train loss: 1.781\n",
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 1.539828\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.326625\n",
            "Epoch 2 mean train loss: 1.457\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.264794\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.136057\n",
            "Epoch 3 mean train loss: 1.245\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.233469\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.847930\n",
            "Epoch 4 mean train loss: 1.127\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.108225\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.921185\n",
            "Epoch 5 mean train loss: 1.038\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.373961\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.935268\n",
            "Epoch 6 mean train loss: 0.982\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.694054\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.782421\n",
            "Epoch 7 mean train loss: 0.914\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.899538\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.933431\n",
            "Epoch 8 mean train loss: 0.850\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.583602\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.664035\n",
            "Epoch 9 mean train loss: 0.814\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.622235\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.934845\n",
            "Epoch 10 mean train loss: 0.784\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.932944\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.674540\n",
            "Epoch 11 mean train loss: 0.749\n",
            "Train Epoch: 11 [0/40000 (0%)]\tLoss: 0.833937\n",
            "Train Epoch: 11 [32000/40000 (80%)]\tLoss: 0.766329\n",
            "Epoch 12 mean train loss: 0.720\n",
            "Train Epoch: 12 [0/40000 (0%)]\tLoss: 0.556793\n",
            "Train Epoch: 12 [32000/40000 (80%)]\tLoss: 0.705235\n",
            "Epoch 13 mean train loss: 0.698\n",
            "Train Epoch: 13 [0/40000 (0%)]\tLoss: 0.558194\n",
            "Train Epoch: 13 [32000/40000 (80%)]\tLoss: 0.813632\n",
            "Epoch 14 mean train loss: 0.670\n",
            "Train Epoch: 14 [0/40000 (0%)]\tLoss: 0.490064\n",
            "Train Epoch: 14 [32000/40000 (80%)]\tLoss: 0.670976\n",
            "Epoch 15 mean train loss: 0.650\n",
            "Train Epoch: 15 [0/40000 (0%)]\tLoss: 0.549171\n",
            "Train Epoch: 15 [32000/40000 (80%)]\tLoss: 0.587203\n",
            "Epoch 16 mean train loss: 0.633\n",
            "Train Epoch: 16 [0/40000 (0%)]\tLoss: 0.577395\n",
            "Train Epoch: 16 [32000/40000 (80%)]\tLoss: 0.630299\n",
            "Epoch 17 mean train loss: 0.608\n",
            "Train Epoch: 17 [0/40000 (0%)]\tLoss: 0.653377\n",
            "Train Epoch: 17 [32000/40000 (80%)]\tLoss: 0.501459\n",
            "Epoch 18 mean train loss: 0.599\n",
            "Train Epoch: 18 [0/40000 (0%)]\tLoss: 0.609002\n",
            "Train Epoch: 18 [32000/40000 (80%)]\tLoss: 0.661075\n",
            "Epoch 19 mean train loss: 0.578\n",
            "Train Epoch: 19 [0/40000 (0%)]\tLoss: 0.454971\n",
            "Train Epoch: 19 [32000/40000 (80%)]\tLoss: 0.500798\n",
            "Epoch 20 mean train loss: 0.567\n",
            "Train Epoch: 20 [0/40000 (0%)]\tLoss: 0.547432\n",
            "Train Epoch: 20 [32000/40000 (80%)]\tLoss: 0.581408\n",
            "Epoch 21 mean train loss: 0.554\n",
            "Train Epoch: 21 [0/40000 (0%)]\tLoss: 0.479175\n",
            "Train Epoch: 21 [32000/40000 (80%)]\tLoss: 0.492813\n",
            "Epoch 22 mean train loss: 0.538\n",
            "Train Epoch: 22 [0/40000 (0%)]\tLoss: 0.468831\n",
            "Train Epoch: 22 [32000/40000 (80%)]\tLoss: 0.482150\n",
            "Epoch 23 mean train loss: 0.524\n",
            "Train Epoch: 23 [0/40000 (0%)]\tLoss: 0.647824\n",
            "Train Epoch: 23 [32000/40000 (80%)]\tLoss: 0.552558\n",
            "Epoch 24 mean train loss: 0.513\n",
            "Train Epoch: 24 [0/40000 (0%)]\tLoss: 0.439038\n",
            "Train Epoch: 24 [32000/40000 (80%)]\tLoss: 0.501835\n",
            "Epoch 25 mean train loss: 0.496\n",
            "Train Epoch: 25 [0/40000 (0%)]\tLoss: 0.367348\n",
            "Train Epoch: 25 [32000/40000 (80%)]\tLoss: 0.506851\n",
            "Epoch 26 mean train loss: 0.492\n",
            "Train Epoch: 26 [0/40000 (0%)]\tLoss: 0.542531\n",
            "Train Epoch: 26 [32000/40000 (80%)]\tLoss: 0.654345\n",
            "Epoch 27 mean train loss: 0.478\n",
            "Train Epoch: 27 [0/40000 (0%)]\tLoss: 0.464418\n",
            "Train Epoch: 27 [32000/40000 (80%)]\tLoss: 0.414765\n",
            "Epoch 28 mean train loss: 0.466\n",
            "Train Epoch: 28 [0/40000 (0%)]\tLoss: 0.280577\n",
            "Train Epoch: 28 [32000/40000 (80%)]\tLoss: 0.540595\n",
            "Epoch 29 mean train loss: 0.458\n",
            "Train Epoch: 29 [0/40000 (0%)]\tLoss: 0.577083\n",
            "Train Epoch: 29 [32000/40000 (80%)]\tLoss: 0.640900\n",
            "Epoch 30 mean train loss: 0.441\n",
            "Train Epoch: 30 [0/40000 (0%)]\tLoss: 0.402220\n",
            "Train Epoch: 30 [32000/40000 (80%)]\tLoss: 0.292956\n",
            "Epoch 31 mean train loss: 0.435\n",
            "Train Epoch: 31 [0/40000 (0%)]\tLoss: 0.254855\n",
            "Train Epoch: 31 [32000/40000 (80%)]\tLoss: 0.486872\n",
            "Epoch 32 mean train loss: 0.427\n",
            "Train Epoch: 32 [0/40000 (0%)]\tLoss: 0.618679\n",
            "Train Epoch: 32 [32000/40000 (80%)]\tLoss: 0.371916\n",
            "Epoch 33 mean train loss: 0.421\n",
            "Train Epoch: 33 [0/40000 (0%)]\tLoss: 0.495353\n",
            "Train Epoch: 33 [32000/40000 (80%)]\tLoss: 0.559340\n",
            "Epoch 34 mean train loss: 0.416\n",
            "Train Epoch: 34 [0/40000 (0%)]\tLoss: 0.320961\n",
            "Train Epoch: 34 [32000/40000 (80%)]\tLoss: 0.267599\n",
            "Epoch 35 mean train loss: 0.399\n",
            "Train Epoch: 35 [0/40000 (0%)]\tLoss: 0.383014\n",
            "Train Epoch: 35 [32000/40000 (80%)]\tLoss: 0.572404\n",
            "Epoch 36 mean train loss: 0.403\n",
            "Train Epoch: 36 [0/40000 (0%)]\tLoss: 0.614074\n",
            "Train Epoch: 36 [32000/40000 (80%)]\tLoss: 0.376816\n",
            "Epoch 37 mean train loss: 0.384\n",
            "Train Epoch: 37 [0/40000 (0%)]\tLoss: 0.184739\n",
            "Train Epoch: 37 [32000/40000 (80%)]\tLoss: 0.431313\n",
            "Epoch 38 mean train loss: 0.382\n",
            "Train Epoch: 38 [0/40000 (0%)]\tLoss: 0.388615\n",
            "Train Epoch: 38 [32000/40000 (80%)]\tLoss: 0.369044\n",
            "Epoch 39 mean train loss: 0.376\n",
            "Train Epoch: 39 [0/40000 (0%)]\tLoss: 0.260595\n",
            "Train Epoch: 39 [32000/40000 (80%)]\tLoss: 0.348608\n",
            "Epoch 40 mean train loss: 0.368\n",
            "Train Epoch: 40 [0/40000 (0%)]\tLoss: 0.241825\n",
            "Train Epoch: 40 [32000/40000 (80%)]\tLoss: 0.462948\n",
            "Epoch 41 mean train loss: 0.360\n",
            "Train Epoch: 41 [0/40000 (0%)]\tLoss: 0.414099\n",
            "Train Epoch: 41 [32000/40000 (80%)]\tLoss: 0.425226\n",
            "Epoch 42 mean train loss: 0.352\n",
            "Train Epoch: 42 [0/40000 (0%)]\tLoss: 0.383143\n",
            "Train Epoch: 42 [32000/40000 (80%)]\tLoss: 0.529540\n",
            "Epoch 43 mean train loss: 0.351\n",
            "Train Epoch: 43 [0/40000 (0%)]\tLoss: 0.308932\n",
            "Train Epoch: 43 [32000/40000 (80%)]\tLoss: 0.208656\n",
            "Epoch 44 mean train loss: 0.343\n",
            "Train Epoch: 44 [0/40000 (0%)]\tLoss: 0.412245\n",
            "Train Epoch: 44 [32000/40000 (80%)]\tLoss: 0.484539\n",
            "Epoch 45 mean train loss: 0.331\n",
            "Train Epoch: 45 [0/40000 (0%)]\tLoss: 0.348081\n",
            "Train Epoch: 45 [32000/40000 (80%)]\tLoss: 0.402939\n",
            "Epoch 46 mean train loss: 0.330\n",
            "Train Epoch: 46 [0/40000 (0%)]\tLoss: 0.409692\n",
            "Train Epoch: 46 [32000/40000 (80%)]\tLoss: 0.398609\n",
            "Epoch 47 mean train loss: 0.321\n",
            "Train Epoch: 47 [0/40000 (0%)]\tLoss: 0.302479\n",
            "Train Epoch: 47 [32000/40000 (80%)]\tLoss: 0.420635\n",
            "Epoch 48 mean train loss: 0.320\n",
            "Train Epoch: 48 [0/40000 (0%)]\tLoss: 0.204542\n",
            "Train Epoch: 48 [32000/40000 (80%)]\tLoss: 0.432527\n",
            "Epoch 49 mean train loss: 0.308\n",
            "Train Epoch: 49 [0/40000 (0%)]\tLoss: 0.315031\n",
            "Train Epoch: 49 [32000/40000 (80%)]\tLoss: 0.234095\n",
            "Epoch 50 mean train loss: 0.309\n",
            "Train Epoch: 50 [0/40000 (0%)]\tLoss: 0.288605\n",
            "Train Epoch: 50 [32000/40000 (80%)]\tLoss: 0.254032\n",
            "Epoch 51 mean train loss: 0.304\n",
            "Train Epoch: 51 [0/40000 (0%)]\tLoss: 0.354090\n",
            "Train Epoch: 51 [32000/40000 (80%)]\tLoss: 0.349224\n",
            "Epoch 52 mean train loss: 0.302\n",
            "Train Epoch: 52 [0/40000 (0%)]\tLoss: 0.197819\n",
            "Train Epoch: 52 [32000/40000 (80%)]\tLoss: 0.301098\n",
            "Epoch 53 mean train loss: 0.299\n",
            "Train Epoch: 53 [0/40000 (0%)]\tLoss: 0.341252\n",
            "Train Epoch: 53 [32000/40000 (80%)]\tLoss: 0.231408\n",
            "Epoch 54 mean train loss: 0.291\n",
            "Train Epoch: 54 [0/40000 (0%)]\tLoss: 0.259508\n",
            "Train Epoch: 54 [32000/40000 (80%)]\tLoss: 0.157740\n",
            "Epoch 55 mean train loss: 0.286\n",
            "Train Epoch: 55 [0/40000 (0%)]\tLoss: 0.274743\n",
            "Train Epoch: 55 [32000/40000 (80%)]\tLoss: 0.361265\n",
            "Epoch 56 mean train loss: 0.279\n",
            "Train Epoch: 56 [0/40000 (0%)]\tLoss: 0.294728\n",
            "Train Epoch: 56 [32000/40000 (80%)]\tLoss: 0.380284\n",
            "Epoch 57 mean train loss: 0.278\n",
            "Train Epoch: 57 [0/40000 (0%)]\tLoss: 0.284804\n",
            "Train Epoch: 57 [32000/40000 (80%)]\tLoss: 0.279337\n",
            "Epoch 58 mean train loss: 0.268\n",
            "Train Epoch: 58 [0/40000 (0%)]\tLoss: 0.323154\n",
            "Train Epoch: 58 [32000/40000 (80%)]\tLoss: 0.203567\n",
            "Epoch 59 mean train loss: 0.265\n",
            "Train Epoch: 59 [0/40000 (0%)]\tLoss: 0.290033\n",
            "Train Epoch: 59 [32000/40000 (80%)]\tLoss: 0.284110\n",
            "Epoch 60 mean train loss: 0.262\n",
            "Train Epoch: 60 [0/40000 (0%)]\tLoss: 0.296102\n",
            "Train Epoch: 60 [32000/40000 (80%)]\tLoss: 0.438076\n",
            "Epoch 61 mean train loss: 0.261\n",
            "Train Epoch: 61 [0/40000 (0%)]\tLoss: 0.268284\n",
            "Train Epoch: 61 [32000/40000 (80%)]\tLoss: 0.278198\n",
            "Epoch 62 mean train loss: 0.250\n",
            "Train Epoch: 62 [0/40000 (0%)]\tLoss: 0.339016\n",
            "Train Epoch: 62 [32000/40000 (80%)]\tLoss: 0.202907\n",
            "Epoch 63 mean train loss: 0.254\n",
            "Train Epoch: 63 [0/40000 (0%)]\tLoss: 0.156620\n",
            "Train Epoch: 63 [32000/40000 (80%)]\tLoss: 0.215310\n",
            "Epoch 64 mean train loss: 0.247\n",
            "Train Epoch: 64 [0/40000 (0%)]\tLoss: 0.252670\n",
            "Train Epoch: 64 [32000/40000 (80%)]\tLoss: 0.187379\n",
            "Epoch 65 mean train loss: 0.246\n",
            "Train Epoch: 65 [0/40000 (0%)]\tLoss: 0.147908\n",
            "Train Epoch: 65 [32000/40000 (80%)]\tLoss: 0.317110\n",
            "Epoch 66 mean train loss: 0.237\n",
            "Train Epoch: 66 [0/40000 (0%)]\tLoss: 0.227976\n",
            "Train Epoch: 66 [32000/40000 (80%)]\tLoss: 0.224252\n",
            "Epoch 67 mean train loss: 0.240\n",
            "Train Epoch: 67 [0/40000 (0%)]\tLoss: 0.148419\n",
            "Train Epoch: 67 [32000/40000 (80%)]\tLoss: 0.339156\n",
            "Epoch 68 mean train loss: 0.235\n",
            "Train Epoch: 68 [0/40000 (0%)]\tLoss: 0.306683\n",
            "Train Epoch: 68 [32000/40000 (80%)]\tLoss: 0.172262\n",
            "Epoch 69 mean train loss: 0.228\n",
            "Train Epoch: 69 [0/40000 (0%)]\tLoss: 0.166400\n",
            "Train Epoch: 69 [32000/40000 (80%)]\tLoss: 0.186929\n",
            "Epoch 70 mean train loss: 0.230\n",
            "Train Epoch: 70 [0/40000 (0%)]\tLoss: 0.277054\n",
            "Train Epoch: 70 [32000/40000 (80%)]\tLoss: 0.271399\n",
            "Epoch 71 mean train loss: 0.223\n",
            "Train Epoch: 71 [0/40000 (0%)]\tLoss: 0.110733\n",
            "Train Epoch: 71 [32000/40000 (80%)]\tLoss: 0.277745\n",
            "Epoch 72 mean train loss: 0.225\n",
            "Train Epoch: 72 [0/40000 (0%)]\tLoss: 0.166125\n",
            "Train Epoch: 72 [32000/40000 (80%)]\tLoss: 0.203594\n",
            "Epoch 73 mean train loss: 0.216\n",
            "Train Epoch: 73 [0/40000 (0%)]\tLoss: 0.052124\n",
            "Train Epoch: 73 [32000/40000 (80%)]\tLoss: 0.109589\n",
            "Epoch 74 mean train loss: 0.218\n",
            "Train Epoch: 74 [0/40000 (0%)]\tLoss: 0.277331\n",
            "Train Epoch: 74 [32000/40000 (80%)]\tLoss: 0.143566\n",
            "Epoch 75 mean train loss: 0.216\n",
            "Train Epoch: 75 [0/40000 (0%)]\tLoss: 0.256405\n",
            "Train Epoch: 75 [32000/40000 (80%)]\tLoss: 0.205617\n",
            "Epoch 76 mean train loss: 0.212\n",
            "Train Epoch: 76 [0/40000 (0%)]\tLoss: 0.195101\n",
            "Train Epoch: 76 [32000/40000 (80%)]\tLoss: 0.160201\n",
            "Epoch 77 mean train loss: 0.205\n",
            "Train Epoch: 77 [0/40000 (0%)]\tLoss: 0.272718\n",
            "Train Epoch: 77 [32000/40000 (80%)]\tLoss: 0.248075\n",
            "Epoch 78 mean train loss: 0.206\n",
            "Train Epoch: 78 [0/40000 (0%)]\tLoss: 0.184654\n",
            "Train Epoch: 78 [32000/40000 (80%)]\tLoss: 0.182475\n",
            "Epoch 79 mean train loss: 0.208\n",
            "Train Epoch: 79 [0/40000 (0%)]\tLoss: 0.118783\n",
            "Train Epoch: 79 [32000/40000 (80%)]\tLoss: 0.245311\n",
            "Epoch 80 mean train loss: 0.202\n",
            "Train Epoch: 80 [0/40000 (0%)]\tLoss: 0.176841\n",
            "Train Epoch: 80 [32000/40000 (80%)]\tLoss: 0.110242\n",
            "Epoch 81 mean train loss: 0.197\n",
            "Train Epoch: 81 [0/40000 (0%)]\tLoss: 0.164433\n",
            "Train Epoch: 81 [32000/40000 (80%)]\tLoss: 0.171940\n",
            "Epoch 82 mean train loss: 0.190\n",
            "Train Epoch: 82 [0/40000 (0%)]\tLoss: 0.145056\n",
            "Train Epoch: 82 [32000/40000 (80%)]\tLoss: 0.277104\n",
            "Epoch 83 mean train loss: 0.191\n",
            "Train Epoch: 83 [0/40000 (0%)]\tLoss: 0.257974\n",
            "Train Epoch: 83 [32000/40000 (80%)]\tLoss: 0.191680\n",
            "Epoch 84 mean train loss: 0.192\n",
            "Train Epoch: 84 [0/40000 (0%)]\tLoss: 0.219471\n",
            "Train Epoch: 84 [32000/40000 (80%)]\tLoss: 0.122067\n",
            "Epoch 85 mean train loss: 0.187\n",
            "Train Epoch: 85 [0/40000 (0%)]\tLoss: 0.102086\n",
            "Train Epoch: 85 [32000/40000 (80%)]\tLoss: 0.261173\n",
            "Epoch 86 mean train loss: 0.192\n",
            "Train Epoch: 86 [0/40000 (0%)]\tLoss: 0.124672\n",
            "Train Epoch: 86 [32000/40000 (80%)]\tLoss: 0.188402\n",
            "Epoch 87 mean train loss: 0.183\n",
            "Train Epoch: 87 [0/40000 (0%)]\tLoss: 0.151514\n",
            "Train Epoch: 87 [32000/40000 (80%)]\tLoss: 0.187440\n",
            "Epoch 88 mean train loss: 0.184\n",
            "Train Epoch: 88 [0/40000 (0%)]\tLoss: 0.054214\n",
            "Train Epoch: 88 [32000/40000 (80%)]\tLoss: 0.255882\n",
            "Epoch 89 mean train loss: 0.184\n",
            "Train Epoch: 89 [0/40000 (0%)]\tLoss: 0.305307\n",
            "Train Epoch: 89 [32000/40000 (80%)]\tLoss: 0.088012\n",
            "Epoch 90 mean train loss: 0.177\n",
            "Train Epoch: 90 [0/40000 (0%)]\tLoss: 0.125228\n",
            "Train Epoch: 90 [32000/40000 (80%)]\tLoss: 0.098553\n",
            "Epoch 91 mean train loss: 0.177\n",
            "Train Epoch: 91 [0/40000 (0%)]\tLoss: 0.164008\n",
            "Train Epoch: 91 [32000/40000 (80%)]\tLoss: 0.163507\n",
            "Epoch 92 mean train loss: 0.175\n",
            "Train Epoch: 92 [0/40000 (0%)]\tLoss: 0.172791\n",
            "Train Epoch: 92 [32000/40000 (80%)]\tLoss: 0.278320\n",
            "Epoch 93 mean train loss: 0.169\n",
            "Train Epoch: 93 [0/40000 (0%)]\tLoss: 0.155721\n",
            "Train Epoch: 93 [32000/40000 (80%)]\tLoss: 0.164992\n",
            "Epoch 94 mean train loss: 0.174\n",
            "Train Epoch: 94 [0/40000 (0%)]\tLoss: 0.097710\n",
            "Train Epoch: 94 [32000/40000 (80%)]\tLoss: 0.148778\n",
            "Epoch 95 mean train loss: 0.173\n",
            "Train Epoch: 95 [0/40000 (0%)]\tLoss: 0.088663\n",
            "Train Epoch: 95 [32000/40000 (80%)]\tLoss: 0.062950\n",
            "Epoch 96 mean train loss: 0.170\n",
            "Train Epoch: 96 [0/40000 (0%)]\tLoss: 0.157902\n",
            "Train Epoch: 96 [32000/40000 (80%)]\tLoss: 0.205960\n",
            "Epoch 97 mean train loss: 0.162\n",
            "Train Epoch: 97 [0/40000 (0%)]\tLoss: 0.201318\n",
            "Train Epoch: 97 [32000/40000 (80%)]\tLoss: 0.142384\n",
            "Epoch 98 mean train loss: 0.165\n",
            "Train Epoch: 98 [0/40000 (0%)]\tLoss: 0.111238\n",
            "Train Epoch: 98 [32000/40000 (80%)]\tLoss: 0.142656\n",
            "Epoch 99 mean train loss: 0.159\n",
            "Train Epoch: 99 [0/40000 (0%)]\tLoss: 0.165604\n",
            "Train Epoch: 99 [32000/40000 (80%)]\tLoss: 0.413567\n",
            "Epoch 100 mean train loss: 0.161\n",
            "\\Test set: Average loss: 104.1668, Accuracy: 8272/10000 (83%)\n",
            "\n",
            "Test accuracy: 0.827\n",
            "\\Test set: Average loss: 104.1668, Accuracy: 8272/10000 (83%)\n",
            "\n",
            "Test accuracy with early stopping: 0.827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18()\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-04)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0)\n",
        "best_accuracy = 0\n",
        "model_state_dict = None\n",
        "for epoch in range(100):\n",
        "    train_loss, val_loss, val_acc = train_and_eval(net, device, trainloader, valloader, optimizer, epoch)\n",
        "    print(f\"Epoch {epoch + 1} mean train loss: {train_loss:.3f}\")\n",
        "    if val_acc > best_accuracy:\n",
        "        best_accuracy = val_acc\n",
        "        model_state_dict = net.state_dict()\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "test_loss, test_acc = test(net, device, testloader)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "net.load_state_dict(model_state_dict)\n",
        "test_loss, test_acc = test(net, device, testloader)\n",
        "print(f\"Test accuracy with early stopping: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMmLNJfRJYoa",
        "outputId": "3091dfc1-0cd0-42c4-d48d-b969021dc1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/40000 (0%)]\tLoss: 2.559731\n",
            "Train Epoch: 0 [32000/40000 (80%)]\tLoss: 1.596507\n",
            "Epoch 1 mean train loss: 1.785\n",
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 1.502620\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.775521\n",
            "Epoch 2 mean train loss: 1.433\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.374968\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.042971\n",
            "Epoch 3 mean train loss: 1.235\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.065932\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.895261\n",
            "Epoch 4 mean train loss: 1.116\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.827065\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.092439\n",
            "Epoch 5 mean train loss: 1.027\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.141567\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.001639\n",
            "Epoch 6 mean train loss: 0.960\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.898122\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.557301\n",
            "Epoch 7 mean train loss: 0.801\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.744496\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.640716\n",
            "Epoch 8 mean train loss: 0.767\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.667495\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.732576\n",
            "Epoch 9 mean train loss: 0.745\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.715451\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.629843\n",
            "Epoch 10 mean train loss: 0.735\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.534831\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.622613\n",
            "Epoch 11 mean train loss: 0.723\n",
            "Train Epoch: 11 [0/40000 (0%)]\tLoss: 0.806573\n",
            "Train Epoch: 11 [32000/40000 (80%)]\tLoss: 0.617531\n",
            "Epoch 12 mean train loss: 0.703\n",
            "Train Epoch: 12 [0/40000 (0%)]\tLoss: 0.682898\n",
            "Train Epoch: 12 [32000/40000 (80%)]\tLoss: 0.882098\n",
            "Epoch 13 mean train loss: 0.700\n",
            "Train Epoch: 13 [0/40000 (0%)]\tLoss: 0.799546\n",
            "Train Epoch: 13 [32000/40000 (80%)]\tLoss: 0.711709\n",
            "Epoch 14 mean train loss: 0.693\n",
            "Train Epoch: 14 [0/40000 (0%)]\tLoss: 0.838541\n",
            "Train Epoch: 14 [32000/40000 (80%)]\tLoss: 0.797344\n",
            "Epoch 15 mean train loss: 0.692\n",
            "Train Epoch: 15 [0/40000 (0%)]\tLoss: 0.733258\n",
            "Train Epoch: 15 [32000/40000 (80%)]\tLoss: 0.721678\n",
            "Epoch 16 mean train loss: 0.692\n",
            "Train Epoch: 16 [0/40000 (0%)]\tLoss: 0.624481\n",
            "Train Epoch: 16 [32000/40000 (80%)]\tLoss: 0.478038\n",
            "Epoch 17 mean train loss: 0.695\n",
            "Train Epoch: 17 [0/40000 (0%)]\tLoss: 0.760416\n",
            "Train Epoch: 17 [32000/40000 (80%)]\tLoss: 0.622098\n",
            "Epoch 18 mean train loss: 0.694\n",
            "Train Epoch: 18 [0/40000 (0%)]\tLoss: 0.408218\n",
            "Train Epoch: 18 [32000/40000 (80%)]\tLoss: 0.876604\n",
            "Epoch 19 mean train loss: 0.688\n",
            "Train Epoch: 19 [0/40000 (0%)]\tLoss: 0.795354\n",
            "Train Epoch: 19 [32000/40000 (80%)]\tLoss: 0.869498\n",
            "Epoch 20 mean train loss: 0.692\n",
            "Train Epoch: 20 [0/40000 (0%)]\tLoss: 0.562789\n",
            "Train Epoch: 20 [32000/40000 (80%)]\tLoss: 0.649958\n",
            "Epoch 21 mean train loss: 0.700\n",
            "Train Epoch: 21 [0/40000 (0%)]\tLoss: 0.745979\n",
            "Train Epoch: 21 [32000/40000 (80%)]\tLoss: 0.762715\n",
            "Epoch 22 mean train loss: 0.693\n",
            "Train Epoch: 22 [0/40000 (0%)]\tLoss: 0.743818\n",
            "Train Epoch: 22 [32000/40000 (80%)]\tLoss: 0.642382\n",
            "Epoch 23 mean train loss: 0.695\n",
            "Train Epoch: 23 [0/40000 (0%)]\tLoss: 0.778898\n",
            "Train Epoch: 23 [32000/40000 (80%)]\tLoss: 0.595517\n",
            "Epoch 24 mean train loss: 0.691\n",
            "Train Epoch: 24 [0/40000 (0%)]\tLoss: 0.830496\n",
            "Train Epoch: 24 [32000/40000 (80%)]\tLoss: 0.667671\n",
            "Epoch 25 mean train loss: 0.692\n",
            "Train Epoch: 25 [0/40000 (0%)]\tLoss: 0.905582\n",
            "Train Epoch: 25 [32000/40000 (80%)]\tLoss: 0.800765\n",
            "Epoch 26 mean train loss: 0.695\n",
            "Train Epoch: 26 [0/40000 (0%)]\tLoss: 0.570322\n",
            "Train Epoch: 26 [32000/40000 (80%)]\tLoss: 0.853606\n",
            "Epoch 27 mean train loss: 0.692\n",
            "Train Epoch: 27 [0/40000 (0%)]\tLoss: 0.663386\n",
            "Train Epoch: 27 [32000/40000 (80%)]\tLoss: 0.492922\n",
            "Epoch 28 mean train loss: 0.690\n",
            "Train Epoch: 28 [0/40000 (0%)]\tLoss: 0.702168\n",
            "Train Epoch: 28 [32000/40000 (80%)]\tLoss: 0.505098\n",
            "Epoch 29 mean train loss: 0.693\n",
            "Train Epoch: 29 [0/40000 (0%)]\tLoss: 0.695408\n",
            "Train Epoch: 29 [32000/40000 (80%)]\tLoss: 0.827306\n",
            "Epoch 30 mean train loss: 0.691\n",
            "Train Epoch: 30 [0/40000 (0%)]\tLoss: 0.692033\n",
            "Train Epoch: 30 [32000/40000 (80%)]\tLoss: 0.646317\n",
            "Epoch 31 mean train loss: 0.689\n",
            "Train Epoch: 31 [0/40000 (0%)]\tLoss: 0.790934\n",
            "Train Epoch: 31 [32000/40000 (80%)]\tLoss: 0.550184\n",
            "Epoch 32 mean train loss: 0.689\n",
            "Train Epoch: 32 [0/40000 (0%)]\tLoss: 0.883946\n",
            "Train Epoch: 32 [32000/40000 (80%)]\tLoss: 0.689102\n",
            "Epoch 33 mean train loss: 0.689\n",
            "Train Epoch: 33 [0/40000 (0%)]\tLoss: 0.652177\n",
            "Train Epoch: 33 [32000/40000 (80%)]\tLoss: 0.907235\n",
            "Epoch 34 mean train loss: 0.693\n",
            "Train Epoch: 34 [0/40000 (0%)]\tLoss: 0.884741\n",
            "Train Epoch: 34 [32000/40000 (80%)]\tLoss: 0.674771\n",
            "Epoch 35 mean train loss: 0.692\n",
            "Train Epoch: 35 [0/40000 (0%)]\tLoss: 0.585097\n",
            "Train Epoch: 35 [32000/40000 (80%)]\tLoss: 0.632183\n",
            "Epoch 36 mean train loss: 0.690\n",
            "Train Epoch: 36 [0/40000 (0%)]\tLoss: 0.617139\n",
            "Train Epoch: 36 [32000/40000 (80%)]\tLoss: 0.711476\n",
            "Epoch 37 mean train loss: 0.688\n",
            "Train Epoch: 37 [0/40000 (0%)]\tLoss: 0.640363\n",
            "Train Epoch: 37 [32000/40000 (80%)]\tLoss: 0.587803\n",
            "Epoch 38 mean train loss: 0.693\n",
            "Train Epoch: 38 [0/40000 (0%)]\tLoss: 0.520366\n",
            "Train Epoch: 38 [32000/40000 (80%)]\tLoss: 0.819559\n",
            "Epoch 39 mean train loss: 0.688\n",
            "Train Epoch: 39 [0/40000 (0%)]\tLoss: 0.662084\n",
            "Train Epoch: 39 [32000/40000 (80%)]\tLoss: 1.057635\n",
            "Epoch 40 mean train loss: 0.690\n",
            "Train Epoch: 40 [0/40000 (0%)]\tLoss: 0.831898\n",
            "Train Epoch: 40 [32000/40000 (80%)]\tLoss: 0.748586\n",
            "Epoch 41 mean train loss: 0.693\n",
            "Train Epoch: 41 [0/40000 (0%)]\tLoss: 0.698260\n",
            "Train Epoch: 41 [32000/40000 (80%)]\tLoss: 0.709959\n",
            "Epoch 42 mean train loss: 0.691\n",
            "Train Epoch: 42 [0/40000 (0%)]\tLoss: 0.615075\n",
            "Train Epoch: 42 [32000/40000 (80%)]\tLoss: 0.716831\n",
            "Epoch 43 mean train loss: 0.686\n",
            "Train Epoch: 43 [0/40000 (0%)]\tLoss: 0.639420\n",
            "Train Epoch: 43 [32000/40000 (80%)]\tLoss: 0.687201\n",
            "Epoch 44 mean train loss: 0.693\n",
            "Train Epoch: 44 [0/40000 (0%)]\tLoss: 0.894761\n",
            "Train Epoch: 44 [32000/40000 (80%)]\tLoss: 0.695721\n",
            "Epoch 45 mean train loss: 0.696\n",
            "Train Epoch: 45 [0/40000 (0%)]\tLoss: 0.674012\n",
            "Train Epoch: 45 [32000/40000 (80%)]\tLoss: 0.651506\n",
            "Epoch 46 mean train loss: 0.693\n",
            "Train Epoch: 46 [0/40000 (0%)]\tLoss: 0.617508\n",
            "Train Epoch: 46 [32000/40000 (80%)]\tLoss: 0.516346\n",
            "Epoch 47 mean train loss: 0.689\n",
            "Train Epoch: 47 [0/40000 (0%)]\tLoss: 0.663803\n",
            "Train Epoch: 47 [32000/40000 (80%)]\tLoss: 0.774835\n",
            "Epoch 48 mean train loss: 0.687\n",
            "Train Epoch: 48 [0/40000 (0%)]\tLoss: 0.601585\n",
            "Train Epoch: 48 [32000/40000 (80%)]\tLoss: 0.553720\n",
            "Epoch 49 mean train loss: 0.697\n",
            "Train Epoch: 49 [0/40000 (0%)]\tLoss: 0.466607\n",
            "Train Epoch: 49 [32000/40000 (80%)]\tLoss: 0.689501\n",
            "Epoch 50 mean train loss: 0.694\n",
            "Train Epoch: 50 [0/40000 (0%)]\tLoss: 0.671855\n",
            "Train Epoch: 50 [32000/40000 (80%)]\tLoss: 0.527949\n",
            "Epoch 51 mean train loss: 0.689\n",
            "Train Epoch: 51 [0/40000 (0%)]\tLoss: 0.627872\n",
            "Train Epoch: 51 [32000/40000 (80%)]\tLoss: 0.667345\n",
            "Epoch 52 mean train loss: 0.694\n",
            "Train Epoch: 52 [0/40000 (0%)]\tLoss: 0.665032\n",
            "Train Epoch: 52 [32000/40000 (80%)]\tLoss: 0.581088\n",
            "Epoch 53 mean train loss: 0.695\n",
            "Train Epoch: 53 [0/40000 (0%)]\tLoss: 0.592743\n",
            "Train Epoch: 53 [32000/40000 (80%)]\tLoss: 0.734965\n",
            "Epoch 54 mean train loss: 0.688\n",
            "Train Epoch: 54 [0/40000 (0%)]\tLoss: 0.621069\n",
            "Train Epoch: 54 [32000/40000 (80%)]\tLoss: 0.621772\n",
            "Epoch 55 mean train loss: 0.693\n",
            "Train Epoch: 55 [0/40000 (0%)]\tLoss: 0.597885\n",
            "Train Epoch: 55 [32000/40000 (80%)]\tLoss: 0.664937\n",
            "Epoch 56 mean train loss: 0.692\n",
            "Train Epoch: 56 [0/40000 (0%)]\tLoss: 0.892043\n",
            "Train Epoch: 56 [32000/40000 (80%)]\tLoss: 0.679718\n",
            "Epoch 57 mean train loss: 0.696\n",
            "Train Epoch: 57 [0/40000 (0%)]\tLoss: 0.556446\n",
            "Train Epoch: 57 [32000/40000 (80%)]\tLoss: 0.674521\n",
            "Epoch 58 mean train loss: 0.693\n",
            "Train Epoch: 58 [0/40000 (0%)]\tLoss: 0.512018\n",
            "Train Epoch: 58 [32000/40000 (80%)]\tLoss: 0.829938\n",
            "Epoch 59 mean train loss: 0.696\n",
            "Train Epoch: 59 [0/40000 (0%)]\tLoss: 0.710298\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}