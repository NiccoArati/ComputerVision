{"cells":[{"cell_type":"markdown","metadata":{"id":"5l5xxcxd0vpW"},"source":["# Using pre-trained CNN\n","\n","In this lab, we will see:\n","\n","- Zero-shot performance of pre-trained backbone\n","- Use pre-trained CNN as backbone\n","- Fine-tuning the pre-trained CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tkm5SfEo1c1r"},"outputs":[],"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchvision\n","import torchvision.models as models\n","import torchvision.transforms as transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Oh2yVOi3Lie"},"outputs":[],"source":["batch_size = 64\n","lr = 0.01\n","epochs = 10\n","device = torch.device(\"cuda\") # to use the GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWJUU9yp0vHp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0019a99b-7745-43ac-e050-c574a92e9e77","executionInfo":{"status":"ok","timestamp":1730129727772,"user_tz":-60,"elapsed":10968,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:05<00:00, 29.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","# create a split for train/validation. We can use early stop\n","trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000])\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2,\n","                                          drop_last=True)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n","                                          shuffle=False, num_workers=2,\n","                                          drop_last=False)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2,\n","                                          drop_last=False)\n"]},{"cell_type":"markdown","source":["## Load a pre-defined network with pretrained weights\n","\n"],"metadata":{"id":"mkiBrlvS1utb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uav1QxKwUXE5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"28edbef6-8504-4195-c10e-183f5c14ebfd","executionInfo":{"status":"ok","timestamp":1730129729182,"user_tz":-60,"elapsed":1420,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 103MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":4}],"source":["net = models.resnet18(pretrained=True)\n","# override the fc layer of the network since it is of 1000 classes by default (ImageNet)\n","net.fc = nn.Linear(512, 10)\n","net.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aWSNZ5Hf18Kn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c69b4fb4-9aea-44f0-e3a5-a295dd15274b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["11181642"]},"metadata":{},"execution_count":5}],"source":["# count the trainable parameters of the model\n","def count_trainable_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","count_trainable_parameters(net)"]},{"cell_type":"code","source":["# frozen all the weights of the network, except for fc ones\n","for param in net.parameters():\n","    param.requires_grad = False\n","net.fc.weight.requires_grad = True\n","net.fc.bias.requires_grad = True\n","count_trainable_parameters(net)"],"metadata":{"id":"3uNOIDDs1_QD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be79ae20-9469-4864-96fd-425f3416dfed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5130"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UH7STI6m0dg9"},"outputs":[],"source":["# define train and test function\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    losses = []\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 500 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","        losses.append(loss.item())\n","    return np.mean(losses)\n","\n","def test(model, device, test_loader, val=False):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += criterion(output, target).item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    mode = \"Val\" if val else \"Test\"\n","    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        mode,\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    test_acc = correct / len(test_loader.dataset)\n","    return test_loss, test_acc"]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-04)"],"metadata":{"id":"b3SqSMwo4qp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqA_UXQZ2zc8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cc988149-211c-43af-c5e1-b05097f689bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.827748\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 2.448497\n","\\Val set: Average loss: 331.4472, Accuracy: 3523/10000 (35%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 2.100048\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.932522\n","\\Val set: Average loss: 333.1411, Accuracy: 3540/10000 (35%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 2.232368\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.597827\n","\\Val set: Average loss: 338.2413, Accuracy: 3388/10000 (34%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 2.136764\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 2.050173\n","\\Val set: Average loss: 309.4349, Accuracy: 3679/10000 (37%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.989875\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 2.635054\n","\\Val set: Average loss: 346.1276, Accuracy: 3300/10000 (33%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 2.366601\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 2.508025\n","\\Val set: Average loss: 324.6189, Accuracy: 3504/10000 (35%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 2.172338\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.742405\n","\\Val set: Average loss: 327.5846, Accuracy: 3550/10000 (36%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.795587\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 1.761458\n","\\Val set: Average loss: 315.0072, Accuracy: 3676/10000 (37%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 2.310194\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 2.003468\n","\\Val set: Average loss: 337.2147, Accuracy: 3438/10000 (34%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 2.125901\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 1.664424\n","\\Val set: Average loss: 321.2299, Accuracy: 3531/10000 (35%)\n","\n"]}],"source":["# the main loop\n","train_losses = []\n","val_losses = []\n","val_accuracies = []\n","model_state_dict = None\n","\n","for epoch in range(1, epochs + 1):\n","    train_loss = train(net, device, trainloader, optimizer, epoch)\n","    train_losses.append(train_loss)\n","    val_loss, val_acc = test(net, device, valloader, val=True)\n","    val_losses.append(val_loss)\n","    val_accuracies.append(val_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4iwFGBFz6FUH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6cc4813e-268b-4082-8104-9fe220144c12"},"outputs":[{"output_type":"stream","name":"stdout","text":["\\Test set: Average loss: 343.2623, Accuracy: 3477/10000 (35%)\n","\n"]}],"source":["test_loss, test_acc = test(net, device, testloader)"]},{"cell_type":"markdown","source":["## Add additional layer to the pre-trained model\n"],"metadata":{"id":"oo5XBPHC5ePQ"}},{"cell_type":"code","source":["fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","net.fc = nn.Linear(128, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    net.fc\n",")\n","net.to(device)"],"metadata":{"id":"Z1UXzoW65drQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d939b0b0-bb1f-409e-e1db-e849624c8497"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Sequential(\n","    (0): Linear(in_features=512, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## Fine-tuning some part of the CNN (not only the classifier)"],"metadata":{"id":"otJejt0A1f9-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4nO6OAdXsXW"},"outputs":[],"source":["# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n","\n","# Assuming you are using an Adam optimizer\n","optimizer = torch.optim.SGD([layer4_params, fc_params], momentum=0.9, weight_decay=1e-04)"]},{"cell_type":"code","source":["def get_results(net, trainloader, optimizer, valloader, testloader, device):\n","    train_losses = []\n","    val_losses = []\n","    val_accuracies = []\n","    model_state_dict = None\n","\n","    for epoch in range(1, epochs + 1):\n","        train_loss = train(net, device, trainloader, optimizer, epoch)\n","        train_losses.append(train_loss)\n","        val_loss, val_acc = test(net, device, valloader, val=True)\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_acc)\n","\n","    test_loss, test_acc = test(net, device, testloader)\n","    return train_losses, val_losses, val_accuracies, test_loss, test_acc"],"metadata":{"id":"3qrgmltML-yd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"id":"502_-PVVMvqU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e38b5531-d016-4037-c2f9-1910118d9c88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.315609\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 2.066176\n","\\Val set: Average loss: 286.7457, Accuracy: 3449/10000 (34%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.550293\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.689312\n","\\Val set: Average loss: 244.8259, Accuracy: 4245/10000 (42%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.908844\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.633983\n","\\Val set: Average loss: 218.5828, Accuracy: 5079/10000 (51%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.688830\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.447590\n","\\Val set: Average loss: 202.0690, Accuracy: 5539/10000 (55%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.258109\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.335482\n","\\Val set: Average loss: 192.5636, Accuracy: 5813/10000 (58%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.258438\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.341215\n","\\Val set: Average loss: 191.3492, Accuracy: 5800/10000 (58%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 1.481382\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.231183\n","\\Val set: Average loss: 183.2410, Accuracy: 6012/10000 (60%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.097681\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 1.313750\n","\\Val set: Average loss: 179.0106, Accuracy: 6155/10000 (62%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 1.175428\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.232515\n","\\Val set: Average loss: 174.7068, Accuracy: 6202/10000 (62%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 1.106809\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 1.056388\n","\\Val set: Average loss: 168.6194, Accuracy: 6304/10000 (63%)\n","\n","\\Test set: Average loss: 169.8140, Accuracy: 6293/10000 (63%)\n","\n","Test accuracy: 0.629\n"]}]},{"cell_type":"markdown","source":["## Exercise 1\n","\n","How many layers it is better to fine-tune?\n","\n","It is better to update all the weights of the model?"],"metadata":{"id":"kyOs4mx92koz"}},{"cell_type":"code","source":["# Try to add more and more layers to finetuning and check\n","net = models.resnet18(pretrained=True)\n","fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","net.fc = nn.Linear(128, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    net.fc\n",")\n","net.to(device)\n","\n","# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer3 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n","layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n","\n","# Assuming you are using an Adam optimizer\n","optimizer = torch.optim.SGD([layer4_params, layer3_params, fc_params], momentum=0.9, weight_decay=1e-04)"],"metadata":{"id":"muMJ79zgDMQs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"293908ce-bb6a-4bb6-f69a-3589245e6a89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ja0yZUD6DLyo","outputId":"93aa6cad-eefc-4e3c-fbd9-d83f24b65183"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.354466\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.578512\n","\\Val set: Average loss: 222.1107, Accuracy: 5037/10000 (50%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.642929\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.467282\n","\\Val set: Average loss: 174.3838, Accuracy: 6211/10000 (62%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.282034\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.996403\n","\\Val set: Average loss: 159.0620, Accuracy: 6583/10000 (66%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.751311\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.987950\n","\\Val set: Average loss: 146.5400, Accuracy: 6771/10000 (68%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.976102\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.012379\n","\\Val set: Average loss: 137.4820, Accuracy: 7009/10000 (70%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.895779\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.824644\n","\\Val set: Average loss: 136.9796, Accuracy: 7037/10000 (70%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.836829\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.991552\n","\\Val set: Average loss: 129.9981, Accuracy: 7133/10000 (71%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.877216\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.739469\n","\\Val set: Average loss: 126.1389, Accuracy: 7218/10000 (72%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.850266\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.658610\n","\\Val set: Average loss: 123.7831, Accuracy: 7278/10000 (73%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.923842\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.764904\n","\\Val set: Average loss: 121.9613, Accuracy: 7380/10000 (74%)\n","\n","\\Test set: Average loss: 117.7501, Accuracy: 7503/10000 (75%)\n","\n","Test accuracy: 0.750\n"]}]},{"cell_type":"code","source":["# Try to add more and more layers to finetuning and check\n","net = models.resnet18(pretrained=True)\n","fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","net.fc = nn.Linear(128, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    net.fc\n",")\n","net.to(device)\n","\n","# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer3 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer2 parameters\n","for param in net.layer2.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n","layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001}\n","layer2_params = {'params': net.layer2.parameters(), 'lr': 0.001}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n","\n","# Assuming you are using an Adam optimizer\n","optimizer = torch.optim.SGD([layer4_params, layer3_params, layer2_params, fc_params], momentum=0.9, weight_decay=1e-04)"],"metadata":{"id":"qnMwvaHzDig_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_MXYw0sD9G_","outputId":"e8889f98-0674-4957-88d8-dd0fb732d147"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.268929\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.617086\n","\\Val set: Average loss: 200.9513, Accuracy: 5459/10000 (55%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.417658\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.153781\n","\\Val set: Average loss: 147.7013, Accuracy: 6760/10000 (68%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.928498\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.034508\n","\\Val set: Average loss: 131.3643, Accuracy: 7198/10000 (72%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.879185\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.894237\n","\\Val set: Average loss: 120.4690, Accuracy: 7429/10000 (74%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.917641\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.782772\n","\\Val set: Average loss: 117.3681, Accuracy: 7493/10000 (75%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.860103\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.654987\n","\\Val set: Average loss: 112.1097, Accuracy: 7584/10000 (76%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.927552\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.519626\n","\\Val set: Average loss: 103.0345, Accuracy: 7780/10000 (78%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.802130\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.566888\n","\\Val set: Average loss: 103.8277, Accuracy: 7794/10000 (78%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.521907\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.543255\n","\\Val set: Average loss: 101.6844, Accuracy: 7835/10000 (78%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.511361\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.470869\n","\\Val set: Average loss: 101.3696, Accuracy: 7816/10000 (78%)\n","\n","\\Test set: Average loss: 96.6028, Accuracy: 7977/10000 (80%)\n","\n","Test accuracy: 0.798\n"]}]},{"cell_type":"code","source":["# Try to add more and more layers to finetuning and check\n","net = models.resnet18(pretrained=True)\n","fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","net.fc = nn.Linear(128, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    net.fc\n",")\n","net.to(device)\n","\n","# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer3 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer2 parameters\n","for param in net.layer2.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer1 parameters\n","for param in net.layer1.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n","layer3_params = {'params': net.layer3.parameters(), 'lr': 0.001}\n","layer2_params = {'params': net.layer2.parameters(), 'lr': 0.001}\n","layer1_params = {'params': net.layer1.parameters(), 'lr': 0.001}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n","\n","# Assuming you are using an Adam optimizer\n","optimizer = torch.optim.SGD([layer4_params, layer3_params, layer2_params, layer1_params, fc_params], momentum=0.9, weight_decay=1e-04)"],"metadata":{"id":"eGboY-oFD_YP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnPgSNTYEAOR","outputId":"e4ea37ce-4c82-488d-bf33-34d64be88199"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.328807\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.204330\n","\\Val set: Average loss: 219.0783, Accuracy: 4824/10000 (48%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.674671\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.094626\n","\\Val set: Average loss: 149.3416, Accuracy: 6770/10000 (68%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.647139\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.833735\n","\\Val set: Average loss: 133.9396, Accuracy: 7075/10000 (71%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.796413\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.979233\n","\\Val set: Average loss: 116.9873, Accuracy: 7509/10000 (75%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.763214\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.819676\n","\\Val set: Average loss: 120.6367, Accuracy: 7470/10000 (75%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.052742\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.594537\n","\\Val set: Average loss: 105.7238, Accuracy: 7742/10000 (77%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.585527\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.731748\n","\\Val set: Average loss: 101.5547, Accuracy: 7884/10000 (79%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.518415\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.484246\n","\\Val set: Average loss: 100.6822, Accuracy: 7869/10000 (79%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.540674\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.498355\n","\\Val set: Average loss: 99.8254, Accuracy: 7877/10000 (79%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.557110\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.424318\n","\\Val set: Average loss: 94.1711, Accuracy: 8004/10000 (80%)\n","\n","\\Test set: Average loss: 90.8844, Accuracy: 8101/10000 (81%)\n","\n","Test accuracy: 0.810\n"]}]},{"cell_type":"markdown","source":["## Exercise 2\n","\n","Try to change the hyper-parameters of the fine-tuning (e.g. lr of CNN layers and lr of the fc layers) and/or network architecture"],"metadata":{"id":"cUunhER-2y9v"}},{"cell_type":"code","source":["# Model choosen is the one with 2 layer finetuned\n","# Try to add more and more layers to finetuning and check\n","net = models.resnet18(pretrained=True)\n","fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","net.fc = nn.Linear(128, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    net.fc\n",")\n","net.to(device)\n","\n","# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer3 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","# Changing fine tuning parameters\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.01}\n","layer3_params = {'params': net.layer3.parameters(), 'lr': 0.01}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.01}\n","\n","optimizer = torch.optim.SGD([layer4_params, layer3_params, fc_params], momentum=0.9, weight_decay=1e-04)"],"metadata":{"id":"2GdMVMg_DL4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2UVPQZzQeB-","executionInfo":{"status":"ok","timestamp":1730037634188,"user_tz":-60,"elapsed":272070,"user":{"displayName":"Niccolò Arati","userId":"01239790669178690119"}},"outputId":"5ce12fd5-0153-46cb-eb8a-b61e6fcb62ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.331332\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.747249\n","\\Val set: Average loss: 250.2112, Accuracy: 4303/10000 (43%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.407115\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.283584\n","\\Val set: Average loss: 184.9564, Accuracy: 5956/10000 (60%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.272162\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.855307\n","\\Val set: Average loss: 170.5542, Accuracy: 6214/10000 (62%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.328250\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.909702\n","\\Val set: Average loss: 156.9608, Accuracy: 6598/10000 (66%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.268493\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.125003\n","\\Val set: Average loss: 149.0362, Accuracy: 6739/10000 (67%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.216636\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.780520\n","\\Val set: Average loss: 139.3544, Accuracy: 6954/10000 (70%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.904058\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.938787\n","\\Val set: Average loss: 134.9567, Accuracy: 7038/10000 (70%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.656800\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.684684\n","\\Val set: Average loss: 130.8797, Accuracy: 7200/10000 (72%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.943810\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.797804\n","\\Val set: Average loss: 127.3314, Accuracy: 7292/10000 (73%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.594675\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.745184\n","\\Val set: Average loss: 127.7791, Accuracy: 7220/10000 (72%)\n","\n","\\Test set: Average loss: 116.7587, Accuracy: 7469/10000 (75%)\n","\n","Test accuracy: 0.747\n"]}]},{"cell_type":"code","source":["# Model choosen is the one with 2 layer finetuned\n","# Try to add more and more layers to finetuning and check\n","net = models.resnet18(pretrained=True)\n","fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","net.fc = nn.Linear(128, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    net.fc\n",")\n","net.to(device)\n","\n","# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer3 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","# Changing fine tuning parameters\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.1}\n","layer3_params = {'params': net.layer3.parameters(), 'lr': 0.1}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.001}\n","\n","optimizer = torch.optim.SGD([layer4_params, layer3_params, fc_params], momentum=0.9, weight_decay=1e-04)"],"metadata":{"id":"KGQ8yfEkQ1t5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UrdXNsmBQ0PG","executionInfo":{"status":"ok","timestamp":1730037903580,"user_tz":-60,"elapsed":269029,"user":{"displayName":"Niccolò Arati","userId":"01239790669178690119"}},"outputId":"66ec3315-cb3b-4460-d898-f6bdb680fd7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.320852\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.071860\n","\\Val set: Average loss: 142.0381, Accuracy: 6914/10000 (69%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 0.825194\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 0.907253\n","\\Val set: Average loss: 132.4523, Accuracy: 7025/10000 (70%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.907629\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.773398\n","\\Val set: Average loss: 126.4757, Accuracy: 7232/10000 (72%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.718469\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.876853\n","\\Val set: Average loss: 117.3408, Accuracy: 7445/10000 (74%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.595156\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.892248\n","\\Val set: Average loss: 119.4196, Accuracy: 7361/10000 (74%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.525111\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.874647\n","\\Val set: Average loss: 112.8146, Accuracy: 7527/10000 (75%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.708070\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.654972\n","\\Val set: Average loss: 113.6506, Accuracy: 7496/10000 (75%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.551706\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.655596\n","\\Val set: Average loss: 113.1269, Accuracy: 7486/10000 (75%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.703779\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.734932\n","\\Val set: Average loss: 107.8911, Accuracy: 7610/10000 (76%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.568900\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.894813\n","\\Val set: Average loss: 108.2420, Accuracy: 7606/10000 (76%)\n","\n","\\Test set: Average loss: 97.9299, Accuracy: 7807/10000 (78%)\n","\n","Test accuracy: 0.781\n"]}]},{"cell_type":"code","source":["# Model choosen is the one with 2 layer finetuned\n","# Try to add more and more layers to finetuning and check\n","net = models.resnet18(pretrained=True)\n","fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","net.fc = nn.Linear(128, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    net.fc\n",")\n","net.to(device)\n","\n","# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer3 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","# Changing fine tuning parameters\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.1}\n","layer3_params = {'params': net.layer3.parameters(), 'lr': 0.1}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n","\n","optimizer = torch.optim.SGD([layer4_params, layer3_params, fc_params], momentum=0.9, weight_decay=1e-04)"],"metadata":{"id":"wa5eFX8aTdjf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLwUvlqFTiRn","executionInfo":{"status":"ok","timestamp":1730038357996,"user_tz":-60,"elapsed":274913,"user":{"displayName":"Niccolò Arati","userId":"01239790669178690119"}},"outputId":"5c27644b-26ee-42a7-d6e0-9dcebeaa3b80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.351829\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.496685\n","\\Val set: Average loss: 212.5698, Accuracy: 5229/10000 (52%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.306263\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.211226\n","\\Val set: Average loss: 157.4727, Accuracy: 6654/10000 (67%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.209106\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.043417\n","\\Val set: Average loss: 157.2708, Accuracy: 6659/10000 (67%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.832244\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.220288\n","\\Val set: Average loss: 138.9392, Accuracy: 7021/10000 (70%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.752076\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.023524\n","\\Val set: Average loss: 141.5030, Accuracy: 6915/10000 (69%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.900422\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.198344\n","\\Val set: Average loss: 154.7159, Accuracy: 6621/10000 (66%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.922001\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.491289\n","\\Val set: Average loss: 138.0316, Accuracy: 7026/10000 (70%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.157395\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 1.125949\n","\\Val set: Average loss: 130.1837, Accuracy: 7210/10000 (72%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.916762\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.726646\n","\\Val set: Average loss: 140.9762, Accuracy: 6928/10000 (69%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 1.112561\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.639986\n","\\Val set: Average loss: 131.5308, Accuracy: 7179/10000 (72%)\n","\n","\\Test set: Average loss: 119.7609, Accuracy: 7468/10000 (75%)\n","\n","Test accuracy: 0.747\n"]}]},{"cell_type":"code","source":["# Changing architecture\n","# Model choosen is the one with 2 layer finetuned\n","# Try to add more and more layers to finetuning and check\n","net = models.resnet18(pretrained=True)\n","fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","fc2 = nn.Linear(128, 64)\n","fc3 = nn.Linear(64, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    fc2,\n","    nn.ReLU(),\n","    fc3\n",")\n","net.to(device)\n","\n","# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer3 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","# Changing fine tuning parameters\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.1}\n","layer3_params = {'params': net.layer3.parameters(), 'lr': 0.1}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.001}\n","\n","optimizer = torch.optim.SGD([layer4_params, layer3_params, fc_params], momentum=0.9, weight_decay=1e-04)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S3WH34XoTvFy","executionInfo":{"status":"ok","timestamp":1730038817304,"user_tz":-60,"elapsed":604,"user":{"displayName":"Niccolò Arati","userId":"01239790669178690119"}},"outputId":"2c4455bd-0b43-417f-df7e-104193a9728c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GX_yaUjcV8Ag","executionInfo":{"status":"ok","timestamp":1730039097943,"user_tz":-60,"elapsed":273326,"user":{"displayName":"Niccolò Arati","userId":"01239790669178690119"}},"outputId":"a4d3efa1-3086-4f43-fb11-78e892837518"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.328296\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 0.947355\n","\\Val set: Average loss: 144.6357, Accuracy: 6859/10000 (69%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 0.854510\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.095747\n","\\Val set: Average loss: 137.8746, Accuracy: 6914/10000 (69%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.816351\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.997718\n","\\Val set: Average loss: 121.7115, Accuracy: 7315/10000 (73%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.874359\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.930420\n","\\Val set: Average loss: 118.5386, Accuracy: 7343/10000 (73%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.810052\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.926476\n","\\Val set: Average loss: 117.1844, Accuracy: 7461/10000 (75%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.770192\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.645483\n","\\Val set: Average loss: 110.6038, Accuracy: 7575/10000 (76%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.736709\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.883001\n","\\Val set: Average loss: 108.5815, Accuracy: 7651/10000 (77%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.489315\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.916806\n","\\Val set: Average loss: 109.8145, Accuracy: 7581/10000 (76%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.842885\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.631819\n","\\Val set: Average loss: 108.1900, Accuracy: 7675/10000 (77%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.385136\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.632829\n","\\Val set: Average loss: 112.9115, Accuracy: 7547/10000 (75%)\n","\n","\\Test set: Average loss: 99.1942, Accuracy: 7825/10000 (78%)\n","\n","Test accuracy: 0.782\n"]}]},{"cell_type":"code","source":["# Model choosen is the one with 2 layer finetuned\n","# Try to add more and more layers to finetuning and check\n","net = models.resnet18(pretrained=True)\n","fc1 = nn.Linear(512, 128)\n","\n","# Modify the existing fully connected layer (fc)\n","fc2 = nn.Linear(128, 64)\n","fc3 = nn.Linear(64, 32)\n","fc4 = nn.Linear(32, 10)\n","\n","# Replace the model's classifier with a new sequential layer\n","# that includes the new fc1 and the modified fc\n","net.fc = nn.Sequential(\n","    fc1,\n","    nn.ReLU(),   # Optional: Add an activation function like ReLU\n","    fc2,\n","    nn.ReLU(),\n","    fc3,\n","    nn.ReLU(),\n","    fc4\n",")\n","net.to(device)\n","\n","# Unfreeze layer4 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze layer3 parameters\n","for param in net.layer4.parameters():\n","    param.requires_grad = True\n","\n","# Unfreeze fc layer parameters\n","net.fc.requires_grad = True\n","\n","# Setting different learning rates\n","# Changing fine tuning parameters\n","layer4_params = {'params': net.layer4.parameters(), 'lr': 0.1}\n","layer3_params = {'params': net.layer3.parameters(), 'lr': 0.1}\n","fc_params = {'params': net.fc.parameters(), 'lr': 0.001}\n","\n","optimizer = torch.optim.SGD([layer4_params, layer3_params, fc_params], momentum=0.9, weight_decay=1e-04)"],"metadata":{"id":"soCai614V8cX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, _, _, _, test_acc = get_results(net, trainloader, optimizer, valloader, testloader, device)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z14dr0R9WRgm","executionInfo":{"status":"ok","timestamp":1730039370146,"user_tz":-60,"elapsed":272207,"user":{"displayName":"Niccolò Arati","userId":"01239790669178690119"}},"outputId":"eeb58c63-9731-4308-8d06-8209f072ee79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.326657\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 0.945511\n","\\Val set: Average loss: 156.2044, Accuracy: 6628/10000 (66%)\n","\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 0.929094\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 0.901915\n","\\Val set: Average loss: 130.9022, Accuracy: 7175/10000 (72%)\n","\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.846631\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 0.797697\n","\\Val set: Average loss: 131.2744, Accuracy: 7130/10000 (71%)\n","\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.646423\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 0.863809\n","\\Val set: Average loss: 122.2112, Accuracy: 7288/10000 (73%)\n","\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 0.828530\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.741123\n","\\Val set: Average loss: 118.3381, Accuracy: 7385/10000 (74%)\n","\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 0.616480\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 0.658790\n","\\Val set: Average loss: 116.2314, Accuracy: 7420/10000 (74%)\n","\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.527205\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.696838\n","\\Val set: Average loss: 113.3391, Accuracy: 7551/10000 (76%)\n","\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.738758\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.617564\n","\\Val set: Average loss: 115.1700, Accuracy: 7478/10000 (75%)\n","\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.636509\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 0.451606\n","\\Val set: Average loss: 108.3399, Accuracy: 7612/10000 (76%)\n","\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.445328\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.613426\n","\\Val set: Average loss: 109.9229, Accuracy: 7581/10000 (76%)\n","\n","\\Test set: Average loss: 97.5701, Accuracy: 7865/10000 (79%)\n","\n","Test accuracy: 0.786\n"]}]},{"cell_type":"markdown","source":["## Exercise 3\n","\n","Try to implement the model selection strategy (also known as early stopping) based on the validation accuracy on cifar10.\n","\n","Consider using the two following command to respectively save and load the state of all the parameters of the model in a moment."],"metadata":{"id":"GuygmHB43UHH"}},{"cell_type":"code","source":["# save all the parameters of the model\n","model_state_dict = net.state_dict()\n","\n","# load saved weights on the model\n","net.load_state_dict(model_state_dict)"],"metadata":{"id":"uOhllTzr32Ld","colab":{"base_uri":"https://localhost:8080/"},"outputId":"95485633-cd28-49c5-e7aa-6b51d74eb818"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["def train_and_eval(model, device, train_loader, valloader, optimizer, epoch):\n","    model.train()\n","    losses = []\n","    val_losses = []\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 500 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","        losses.append(loss.item())\n","\n","    model.eval()\n","    with torch.no_grad():\n","        val_corr = 0\n","        for batch_idx, (data, target) in enumerate(valloader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss_val = criterion(output, target)\n","            val_losses.append(loss_val.item())\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            val_corr += pred.eq(target.view_as(pred)).sum().item()\n","\n","    val_accuracy = val_corr / 10000 # Validation set has 10000 elements\n","    return np.mean(losses), np.mean(val_losses), val_accuracy"],"metadata":{"id":"iRk9OPHzF2qk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = models.resnet18()\n","net.fc = nn.Linear(512, 10)\n","net.to(device)\n","\n","optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-04)\n","best_accuracy = 0\n","best_model_state_dict = None\n","for epoch in range(55):\n","    train_loss, val_loss, val_acc = train_and_eval(net, device, trainloader, valloader, optimizer, epoch)\n","    print(f\"Epoch {epoch + 1} mean train loss: {train_loss:.3f}\")\n","    if val_acc > best_accuracy:\n","        best_accuracy = val_acc\n","        best_model_state_dict = net.state_dict()\n","        print(\"New best model\")\n","\n","test_loss, test_acc = test(net, device, testloader)\n","print(f\"Test accuracy: {test_acc:.3f}\")\n","\n","net.load_state_dict(best_model_state_dict)\n","test_loss, test_acc = test(net, device, testloader)\n","print(f\"Test accuracy with early stopping: {test_acc:.3f}\")"],"metadata":{"id":"ScwmTTmzHSIN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"746b4325-c153-43c7-a0dd-37b09f82cfe0","executionInfo":{"status":"ok","timestamp":1730135133071,"user_tz":-60,"elapsed":1482267,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 0 [0/40000 (0%)]\tLoss: 2.637047\n","Train Epoch: 0 [32000/40000 (80%)]\tLoss: 1.704776\n","Epoch 1 mean train loss: 1.795\n","New best model\n","Train Epoch: 1 [0/40000 (0%)]\tLoss: 1.372432\n","Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.705561\n","Epoch 2 mean train loss: 1.460\n","New best model\n","Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.401211\n","Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.195193\n","Epoch 3 mean train loss: 1.283\n","New best model\n","Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.774399\n","Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.201260\n","Epoch 4 mean train loss: 1.146\n","New best model\n","Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.078824\n","Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.143047\n","Epoch 5 mean train loss: 1.042\n","New best model\n","Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.019892\n","Train Epoch: 5 [32000/40000 (80%)]\tLoss: 0.993923\n","Epoch 6 mean train loss: 0.971\n","New best model\n","Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.134222\n","Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.072113\n","Epoch 7 mean train loss: 0.908\n","Train Epoch: 7 [0/40000 (0%)]\tLoss: 0.824005\n","Train Epoch: 7 [32000/40000 (80%)]\tLoss: 0.484551\n","Epoch 8 mean train loss: 0.864\n","New best model\n","Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.127927\n","Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.914542\n","Epoch 9 mean train loss: 0.821\n","New best model\n","Train Epoch: 9 [0/40000 (0%)]\tLoss: 0.592338\n","Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.018207\n","Epoch 10 mean train loss: 0.778\n","New best model\n","Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.732433\n","Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.693952\n","Epoch 11 mean train loss: 0.761\n","New best model\n","Train Epoch: 11 [0/40000 (0%)]\tLoss: 0.587095\n","Train Epoch: 11 [32000/40000 (80%)]\tLoss: 0.760143\n","Epoch 12 mean train loss: 0.720\n","New best model\n","Train Epoch: 12 [0/40000 (0%)]\tLoss: 0.792090\n","Train Epoch: 12 [32000/40000 (80%)]\tLoss: 0.826366\n","Epoch 13 mean train loss: 0.696\n","New best model\n","Train Epoch: 13 [0/40000 (0%)]\tLoss: 0.660837\n","Train Epoch: 13 [32000/40000 (80%)]\tLoss: 0.485700\n","Epoch 14 mean train loss: 0.670\n","Train Epoch: 14 [0/40000 (0%)]\tLoss: 0.601713\n","Train Epoch: 14 [32000/40000 (80%)]\tLoss: 0.699944\n","Epoch 15 mean train loss: 0.649\n","New best model\n","Train Epoch: 15 [0/40000 (0%)]\tLoss: 0.559863\n","Train Epoch: 15 [32000/40000 (80%)]\tLoss: 0.531297\n","Epoch 16 mean train loss: 0.629\n","Train Epoch: 16 [0/40000 (0%)]\tLoss: 0.597998\n","Train Epoch: 16 [32000/40000 (80%)]\tLoss: 0.697314\n","Epoch 17 mean train loss: 0.612\n","New best model\n","Train Epoch: 17 [0/40000 (0%)]\tLoss: 0.568717\n","Train Epoch: 17 [32000/40000 (80%)]\tLoss: 0.536301\n","Epoch 18 mean train loss: 0.595\n","Train Epoch: 18 [0/40000 (0%)]\tLoss: 0.576623\n","Train Epoch: 18 [32000/40000 (80%)]\tLoss: 0.589095\n","Epoch 19 mean train loss: 0.574\n","New best model\n","Train Epoch: 19 [0/40000 (0%)]\tLoss: 0.563226\n","Train Epoch: 19 [32000/40000 (80%)]\tLoss: 0.463468\n","Epoch 20 mean train loss: 0.565\n","Train Epoch: 20 [0/40000 (0%)]\tLoss: 0.614857\n","Train Epoch: 20 [32000/40000 (80%)]\tLoss: 0.586094\n","Epoch 21 mean train loss: 0.553\n","New best model\n","Train Epoch: 21 [0/40000 (0%)]\tLoss: 0.662967\n","Train Epoch: 21 [32000/40000 (80%)]\tLoss: 0.610156\n","Epoch 22 mean train loss: 0.541\n","Train Epoch: 22 [0/40000 (0%)]\tLoss: 0.426217\n","Train Epoch: 22 [32000/40000 (80%)]\tLoss: 0.582168\n","Epoch 23 mean train loss: 0.516\n","New best model\n","Train Epoch: 23 [0/40000 (0%)]\tLoss: 0.670274\n","Train Epoch: 23 [32000/40000 (80%)]\tLoss: 0.727888\n","Epoch 24 mean train loss: 0.510\n","New best model\n","Train Epoch: 24 [0/40000 (0%)]\tLoss: 0.564591\n","Train Epoch: 24 [32000/40000 (80%)]\tLoss: 0.341096\n","Epoch 25 mean train loss: 0.496\n","Train Epoch: 25 [0/40000 (0%)]\tLoss: 0.509256\n","Train Epoch: 25 [32000/40000 (80%)]\tLoss: 0.734596\n","Epoch 26 mean train loss: 0.487\n","Train Epoch: 26 [0/40000 (0%)]\tLoss: 0.743467\n","Train Epoch: 26 [32000/40000 (80%)]\tLoss: 0.444081\n","Epoch 27 mean train loss: 0.478\n","Train Epoch: 27 [0/40000 (0%)]\tLoss: 0.552463\n","Train Epoch: 27 [32000/40000 (80%)]\tLoss: 0.578264\n","Epoch 28 mean train loss: 0.462\n","New best model\n","Train Epoch: 28 [0/40000 (0%)]\tLoss: 0.304888\n","Train Epoch: 28 [32000/40000 (80%)]\tLoss: 0.436255\n","Epoch 29 mean train loss: 0.456\n","New best model\n","Train Epoch: 29 [0/40000 (0%)]\tLoss: 0.366450\n","Train Epoch: 29 [32000/40000 (80%)]\tLoss: 0.258564\n","Epoch 30 mean train loss: 0.447\n","Train Epoch: 30 [0/40000 (0%)]\tLoss: 0.484788\n","Train Epoch: 30 [32000/40000 (80%)]\tLoss: 0.329711\n","Epoch 31 mean train loss: 0.435\n","New best model\n","Train Epoch: 31 [0/40000 (0%)]\tLoss: 0.500009\n","Train Epoch: 31 [32000/40000 (80%)]\tLoss: 0.273195\n","Epoch 32 mean train loss: 0.424\n","New best model\n","Train Epoch: 32 [0/40000 (0%)]\tLoss: 0.298367\n","Train Epoch: 32 [32000/40000 (80%)]\tLoss: 0.605399\n","Epoch 33 mean train loss: 0.423\n","Train Epoch: 33 [0/40000 (0%)]\tLoss: 0.264740\n","Train Epoch: 33 [32000/40000 (80%)]\tLoss: 0.186189\n","Epoch 34 mean train loss: 0.412\n","Train Epoch: 34 [0/40000 (0%)]\tLoss: 0.462913\n","Train Epoch: 34 [32000/40000 (80%)]\tLoss: 0.425889\n","Epoch 35 mean train loss: 0.404\n","New best model\n","Train Epoch: 35 [0/40000 (0%)]\tLoss: 0.249616\n","Train Epoch: 35 [32000/40000 (80%)]\tLoss: 0.438961\n","Epoch 36 mean train loss: 0.395\n","Train Epoch: 36 [0/40000 (0%)]\tLoss: 0.343435\n","Train Epoch: 36 [32000/40000 (80%)]\tLoss: 0.404176\n","Epoch 37 mean train loss: 0.386\n","Train Epoch: 37 [0/40000 (0%)]\tLoss: 0.432479\n","Train Epoch: 37 [32000/40000 (80%)]\tLoss: 0.483514\n","Epoch 38 mean train loss: 0.380\n","Train Epoch: 38 [0/40000 (0%)]\tLoss: 0.357185\n","Train Epoch: 38 [32000/40000 (80%)]\tLoss: 0.325693\n","Epoch 39 mean train loss: 0.369\n","Train Epoch: 39 [0/40000 (0%)]\tLoss: 0.226623\n","Train Epoch: 39 [32000/40000 (80%)]\tLoss: 0.410365\n","Epoch 40 mean train loss: 0.367\n","Train Epoch: 40 [0/40000 (0%)]\tLoss: 0.416021\n","Train Epoch: 40 [32000/40000 (80%)]\tLoss: 0.293917\n","Epoch 41 mean train loss: 0.360\n","Train Epoch: 41 [0/40000 (0%)]\tLoss: 0.294580\n","Train Epoch: 41 [32000/40000 (80%)]\tLoss: 0.186817\n","Epoch 42 mean train loss: 0.351\n","Train Epoch: 42 [0/40000 (0%)]\tLoss: 0.376899\n","Train Epoch: 42 [32000/40000 (80%)]\tLoss: 0.394569\n","Epoch 43 mean train loss: 0.345\n","Train Epoch: 43 [0/40000 (0%)]\tLoss: 0.251362\n","Train Epoch: 43 [32000/40000 (80%)]\tLoss: 0.181718\n","Epoch 44 mean train loss: 0.339\n","Train Epoch: 44 [0/40000 (0%)]\tLoss: 0.362902\n","Train Epoch: 44 [32000/40000 (80%)]\tLoss: 0.200655\n","Epoch 45 mean train loss: 0.329\n","New best model\n","Train Epoch: 45 [0/40000 (0%)]\tLoss: 0.274892\n","Train Epoch: 45 [32000/40000 (80%)]\tLoss: 0.290257\n","Epoch 46 mean train loss: 0.330\n","Train Epoch: 46 [0/40000 (0%)]\tLoss: 0.261252\n","Train Epoch: 46 [32000/40000 (80%)]\tLoss: 0.239201\n","Epoch 47 mean train loss: 0.323\n","Train Epoch: 47 [0/40000 (0%)]\tLoss: 0.510970\n","Train Epoch: 47 [32000/40000 (80%)]\tLoss: 0.253770\n","Epoch 48 mean train loss: 0.320\n","Train Epoch: 48 [0/40000 (0%)]\tLoss: 0.171659\n","Train Epoch: 48 [32000/40000 (80%)]\tLoss: 0.375581\n","Epoch 49 mean train loss: 0.312\n","Train Epoch: 49 [0/40000 (0%)]\tLoss: 0.201171\n","Train Epoch: 49 [32000/40000 (80%)]\tLoss: 0.387569\n","Epoch 50 mean train loss: 0.309\n","Train Epoch: 50 [0/40000 (0%)]\tLoss: 0.243595\n","Train Epoch: 50 [32000/40000 (80%)]\tLoss: 0.312297\n","Epoch 51 mean train loss: 0.306\n","Train Epoch: 51 [0/40000 (0%)]\tLoss: 0.229165\n","Train Epoch: 51 [32000/40000 (80%)]\tLoss: 0.262693\n","Epoch 52 mean train loss: 0.300\n","New best model\n","Train Epoch: 52 [0/40000 (0%)]\tLoss: 0.426276\n","Train Epoch: 52 [32000/40000 (80%)]\tLoss: 0.217174\n","Epoch 53 mean train loss: 0.288\n","Train Epoch: 53 [0/40000 (0%)]\tLoss: 0.183588\n","Train Epoch: 53 [32000/40000 (80%)]\tLoss: 0.222610\n","Epoch 54 mean train loss: 0.284\n","New best model\n","Train Epoch: 54 [0/40000 (0%)]\tLoss: 0.198900\n","Train Epoch: 54 [32000/40000 (80%)]\tLoss: 0.280703\n","Epoch 55 mean train loss: 0.283\n","New best model\n","\\Test set: Average loss: 92.5834, Accuracy: 8215/10000 (82%)\n","\n","Test accuracy: 0.822\n","\\Test set: Average loss: 92.5834, Accuracy: 8215/10000 (82%)\n","\n","Test accuracy with early stopping: 0.822\n"]}]},{"cell_type":"code","source":["# Best Model with scheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.4913998, 0.48215738, 0.44653124), (0.24703224, 0.2434851, 0.26158783)),\n","                                transforms.RandomCrop(32, padding=4),\n","                                transforms.RandomHorizontalFlip()])\n","\n","trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000])\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2,\n","                                          drop_last=True)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n","                                          shuffle=False, num_workers=2,\n","                                          drop_last=False)\n","\n","\n","class Net(nn.Module):\n","    def __init__(self, kernel_size=5, stride=1, padding=0):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, kernel_size, stride, padding)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, kernel_size, stride, padding)\n","        if kernel_size == 5:\n","            if padding == 2:\n","                self.fc1 = nn.Linear(1024, 120)\n","            elif stride == 2:\n","                self.fc1 = nn.Linear(16, 120)\n","            else:\n","                self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        elif kernel_size == 3:\n","            self.fc1 = nn.Linear(576, 120)\n","        elif kernel_size == 1:\n","            self.fc1 = nn.Linear(1024, 120)\n","        else: # kernel_size == 7\n","            if padding == 3:\n","                self.fc1 = nn.Linear(1024, 120)\n","            else:\n","                self.fc1 = nn.Linear(144, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"],"metadata":{"id":"1y20gua0NF8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = Net(kernel_size=5, padding=2).to(device)\n","optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-04)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0)\n","\n","best_accuracy = 0\n","best_model_state_dict = None\n","for epoch in range(30):\n","    train_loss, val_loss, val_acc = train_and_eval(net, device, trainloader, valloader, optimizer, epoch)\n","    print(f\"Epoch {epoch + 1} mean train loss: {train_loss:.3f}\")\n","    if val_acc > best_accuracy:\n","        best_accuracy = val_acc\n","        best_model_state_dict = net.state_dict()\n","        print(\"New best model\")\n","\n","test_loss, test_acc = test(net, device, testloader)\n","print(f\"Test accuracy: {test_acc:.3f}\")\n","\n","net.load_state_dict(best_model_state_dict)\n","test_loss, test_acc = test(net, device, testloader)\n","print(f\"Test accuracy with early stopping: {test_acc:.3f}\")"],"metadata":{"id":"0Ufv09VMxRSq"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}