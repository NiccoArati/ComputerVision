{"cells":[{"cell_type":"markdown","metadata":{"id":"Vg7BGBFjv5SP"},"source":["# Siamese Network\n","\n","\n","\n","---\n","\n","In this session, we are going to implement a Siamese Network.\n","\n","It takes as input two augmented versions of the same image and produces as output two feature vectors one for each version of the image.\n","\n","For simplicity, we will use the same backbone to process the views as in SimCLR paper.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnoJ4nz6t3rN"},"outputs":[],"source":["import numpy as np\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torchvision.io import read_image\n","\n","import torchvision\n","import torchvision.models as models\n","import torchvision.transforms as transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G44sBdg5xSA3","outputId":"43d78799-e015-4f73-cc75-74ae12b428b8","executionInfo":{"status":"ok","timestamp":1731066334268,"user_tz":-60,"elapsed":381,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Identity()\n",")\n"]}],"source":["# you can use a resnet18 as backbone\n","backbone = models.resnet18()\n","\n","#! remember to delete the fc layer (we need just the CNN layers + flatten)\n","backbone.fc = nn.Identity()\n","print(backbone)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEu_h8dhx8Lz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731066950380,"user_tz":-60,"elapsed":570,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"d9d02295-a562-4a94-9851-9753b2367d24"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 512])\n","torch.Size([10, 512])\n"]}],"source":["class SiameseNetSIM(nn.Module):\n","    def __init__(self, backbone):\n","        super().__init__()\n","        self.encoder = backbone\n","\n","    def forward(self, x1, x2):\n","        images = torch.cat((x1, x2), 0)\n","        features = self.encoder(images)\n","        return features\n","\n","    # if pairs are concatenated before, use tihs\n","    '''\n","    def forward(self, x):\n","        return self.encoder(x)\n","    '''\n","\n","class SiameseNetASIM(nn.Module):\n","    def __init__(self, backbone, backbone2):\n","        super().__init__()\n","        self.encoder = backbone\n","        self.encoder2 = backbone2\n","\n","    def forward(self, x1, x2):\n","        features = self.encoder(x1)\n","        features2 = self.encoder2(x2)\n","        return torch.cat((features, features2), 0)\n","\n","\n","# Check output shape\n","features_sim = SiameseNetSIM(backbone)(torch.randn(5, 3, 32, 32), torch.randn(5, 3, 32, 32))\n","print(features_sim.shape)\n","\n","backbone2 = models.resnet18()\n","backbone2.fc = nn.Identity()\n","features_asim = SiameseNetASIM(backbone, backbone2)(torch.randn(5, 3, 32, 32), torch.randn(5, 3, 32, 32))\n","print(features_asim.shape)"]},{"cell_type":"markdown","metadata":{"id":"RlyBgKKzyxyB"},"source":["Let's now use the Dataset which creates the two augmented views for each image from the [past lab session](https://colab.research.google.com/drive/1NJwAFbRiD4MdwWf__6P2Lm0xYk_DNdVu?usp=sharing) and create a loop with forward pass"]},{"cell_type":"code","source":["class CustomImageDataset(Dataset):\n","    def __init__(self, data, targets, transform=None, target_transform=None):\n","        self.imgs = data\n","        self.targets = targets\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","    def __getitem__(self, idx):\n","        img_base = self.imgs[idx]\n","        if isinstance(img_base, str):\n","          img_base = read_image(img_base)\n","        label = self.targets[idx]\n","        if self.transform:\n","            img1 = self.transform(img_base)\n","            img2 = self.transform(img_base)\n","        else:\n","            img1 = img_base\n","            img2 = img_base\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return img1, img2, label\n","\n","\n","data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n","size = 32\n","\n","# simclr DA pipeline\n","s=1\n","color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                  transforms.RandomResizedCrop(size=size),\n","                                  transforms.RandomHorizontalFlip(),\n","                                  transforms.RandomApply([color_jitter], p=0.8),\n","                                  transforms.RandomGrayscale(p=0.2),\n","                                  transforms.GaussianBlur(kernel_size=int(0.1 * size))])\n","\n","# create training set from CustomDataset\n","trainset = CustomImageDataset(data.data, data.targets, transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwS17usNm8GZ","executionInfo":{"status":"ok","timestamp":1731067086273,"user_tz":-60,"elapsed":1888,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"105b75b1-0be1-45a8-9a23-7d4b6ac1c8b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJAgRkIzyUTq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731067092786,"user_tz":-60,"elapsed":2665,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"7909767e-c27b-4937-f666-646d4fa4cd07"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 3, 32, 32])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([128, 512])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([128, 512])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([128, 512])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([128, 512])\n"]}],"source":["dataloader = DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","model = SiameseNetSIM(backbone)\n","\n","for idx, data in enumerate(dataloader):\n","    views1, views2 , targets = data\n","    print(views1.shape)\n","    print(views2.shape)\n","\n","    output = model(views1, views2)\n","    print(output.shape)\n","\n","    if idx == 3:\n","        break"]},{"cell_type":"code","source":["model = SiameseNetASIM(backbone, backbone2)\n","\n","for idx, data in enumerate(dataloader):\n","    views1, views2 , targets = data\n","    print(views1.shape)\n","    print(views2.shape)\n","\n","    output = model(views1, views2)\n","    print(output.shape)\n","\n","    if idx == 3:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umcbBlvnnYcC","executionInfo":{"status":"ok","timestamp":1731067102106,"user_tz":-60,"elapsed":3101,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"c2388640-1cf8-48a7-f9b9-0de6f0373074"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 3, 32, 32])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([128, 512])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([128, 512])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([128, 512])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([64, 3, 32, 32])\n","torch.Size([128, 512])\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1AMkh0q8L5nJScx7v6cMWoK336zqOqDY6","timestamp":1730815646382}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}